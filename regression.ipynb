{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder as ore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cleaned_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.drop(columns=[\"G3\"])\n",
    "y = df['G3']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we will define our features and our target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cleaned_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe=Pipeline([('ore',ore()),('Normalizer',Normalizer()),('Ridge',Ridge())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'denormalize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m pipe\u001b[38;5;241m.\u001b[39mfit(X,y)\n\u001b[0;32m      2\u001b[0m y_pred_norm\u001b[38;5;241m=\u001b[39mpipe\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[1;32m----> 3\u001b[0m y_pred\u001b[38;5;241m=\u001b[39m\u001b[43mdenormalize\u001b[49m(y_pred_norm)\n\u001b[0;32m      4\u001b[0m mse \u001b[38;5;241m=\u001b[39m mean_squared_error(y, y_pred)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSE Score: \u001b[39m\u001b[38;5;124m\"\u001b[39m,mse)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'denormalize' is not defined"
     ]
    }
   ],
   "source": [
    "pipe.fit(X,y)\n",
    "y_pred_norm=pipe.predict(X)\n",
    "y_pred=denormalize(y_pred_norm)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "print(\"MSE Score: \",mse)\n",
    "rs=r2_score(y,y_pred)\n",
    "print(\"R^2 Score: \",rs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we tried Ridge regression with cross "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import numpy as np\n",
    "alphas = [0,0.1, 1.0, 10.0]\n",
    "mean_cv_scores = []\n",
    "for alpha in alphas:\n",
    "    # Define the pipeline with Ridge Regression\n",
    "    pipeline = Pipeline([\n",
    "        ('ore',ore()),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('ridge', Ridge(alpha=alpha))\n",
    "    ])\n",
    "        # Define K-Fold Cross-Validation\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform K-Fold Cross-Validation\n",
    "    cv_scores = cross_val_score(pipeline, X, y, cv=kfold)\n",
    "    mean_cv_score = np.mean(cv_scores)\n",
    "    mean_cv_scores.append(mean_cv_score)\n",
    "best_alpha = alphas[np.argmax(mean_cv_scores)]\n",
    "pipe=Pipeline([('ore',ore()),('Normalizer',Normalizer()),('Ridge',Ridge(alpha=best_alpha))])\n",
    "pipe.fit(X,y)\n",
    "y_pred_norm=pipe.predict(X)\n",
    "y_pred=denormalize(y_pred_norm)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "print(\"MSE Score: \",mse)\n",
    "rs=r2_score(y,y_pred)\n",
    "print(\"R^2 Score: \",rs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we tried only linear regression with the feature we made from previous grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df[['avg_Grade']]\n",
    "y=df['G3']\n",
    "model=LinearRegression()\n",
    "model.fit(X,y)\n",
    "y_pred=model.predict(X)\n",
    "print(mean_squared_error(y_pred,y))\n",
    "print(r2_score(y_pred,y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying Linear Regression with encoding categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "cat = []\n",
    "\n",
    "for i in df:\n",
    "    if (type(df[i][0]) != str):\n",
    "        l.append(i)\n",
    "    else:\n",
    "        cat.append(i)\n",
    "        \n",
    "l.remove(\"G3\")\n",
    "x = df[l].copy()\n",
    "cat = df[cat].copy() # cat is df\n",
    "\n",
    "for i in cat:\n",
    "    unique = cat[i].unique()\n",
    "    uniqueDict = dict()\n",
    "      \n",
    "    for c in range(len(unique)):\n",
    "        uniqueDict[unique[c]] = c\n",
    "        \n",
    "    cat[i] = cat[i].apply(lambda j: uniqueDict[j])\n",
    "        \n",
    "    # for j in cat[i].keys():\n",
    "    #     cat.loc[j, i] = uniqueDict[cat[i][j]]\n",
    "        \n",
    "    cat[i] = cat[i].astype(\"int64\")\n",
    "    x[i] = cat[i].copy()\n",
    "\n",
    "for i in x:\n",
    "    u = np.mean(x[i])\n",
    "    sigma = np.std(x[i])\n",
    "    x[i] = (x[i] - u) / sigma\n",
    "\n",
    "y = df[\"G3\"].copy()\n",
    "\n",
    "for i in y.keys():\n",
    "    u = np.mean(y)\n",
    "    sigma = np.std(y)\n",
    "    y[i] = (y[i] - u) / sigma\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y ,random_state = 104, train_size=0.8, shuffle=True) \n",
    "\n",
    "regr = LinearRegression(fit_intercept = True)\n",
    "regr.fit(x_train, y_train)\n",
    "y_pred = regr.predict(x_test)\n",
    "\n",
    "print(\"MSE Score: \",mean_squared_error(y_test, y_pred))\n",
    "\n",
    "y_pred_train = regr.predict(x_train)\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "print(\"Train R^2 Score:\", train_r2)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred_test = regr.predict(x_test)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "print(\"Test R^2 Score:\", test_r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we selected the columns with the highest correlation with the target and the rejected column from our hypothesis testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "df_copy = df.copy()\n",
    "binary_encoder = LabelEncoder()\n",
    "# These are the columns affecting the target from our hypothesis testing\n",
    "columns = ['school', 'sex', 'address', 'schoolsup', 'higher', 'internet']\n",
    "for col in columns:\n",
    "    df_copy[f'{col}2'] = binary_encoder.fit_transform(df[f'{col}'])\n",
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns2=[f\"{c}2\"for c in columns]\n",
    "# These features have strong correlation with our targeet variable\n",
    "columns2.extend(['avg_Grade',\"Dalc\",\"goout\"])\n",
    "columns2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Grid search to test different parameters for our ridge model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X = df_copy[columns2]\n",
    "y = df_copy['G3']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Ridge()\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 1.0, 10.0],\n",
    "    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best MSE Score:\", -grid_search.best_score_)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Test MSE:\", test_mse)\n",
    "\n",
    "y_pred_train = best_model.predict(X_train)\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "print(\"Train R^2 Score:\", train_r2)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "print(\"Test R^2 Score:\", test_r2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying KNeighborsRegressor with Grid search to find the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "\n",
    "# Assuming df_copy contains your DataFrame and columns2 are the features\n",
    "X = df_copy[columns2]\n",
    "y = df_copy['G3']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create KNN Regressor model\n",
    "model = KNeighborsRegressor()\n",
    "\n",
    "# Define hyperparameters to search\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2]  # 1 for Manhattan distance, 2 for Euclidean distance\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best MSE score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best MSE Score:\", -grid_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Test MSE:\", test_mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = best_model.predict(X_train)\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "print(\"Train R^2 Score:\", train_r2)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "print(\"Test R^2 Score:\", test_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = df.select_dtypes(include=['int64','Float64'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numerical_columns.drop(columns=[\"G3\"])\n",
    "y = numerical_columns[\"G3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying Ensemblle Learning boosting techniqe called Xgboost with hyperparameter tuning and Kfold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Define the XGBoost regressor\n",
    "model = xgb.XGBRegressor()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9],\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1],\n",
    "    'reg_lambda': [0, 0.1, 0.5, 1]\n",
    "}\n",
    "\n",
    "# Perform cross-validation with hyperparameter tuning\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_grid, n_iter=20, scoring='neg_mean_squared_error', cv=kfold, verbose=1, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Print the mean squared error of the best estimator\n",
    "print(\"Best MSE:\", -random_search.best_score_)\n",
    "\n",
    "# You can also access the best estimator directly\n",
    "best_model = random_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the parameters from our best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Initialize XGBoost Regressor\n",
    "model = xgb.XGBRegressor(\n",
    "   n_estimators=100,  \n",
    "   max_depth=5,       \n",
    "   learning_rate=0.1, \n",
    "   subsample=0.8,     \n",
    "   colsample_bytree=0.8, \n",
    "   random_state=42    \n",
    ")\n",
    "\n",
    "# Use a list to store scores\n",
    "mse_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate mean and standard deviation of MSE and R2\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Mean MSE: {:.2f}\".format(mse))\n",
    "print(\"Mean R^2: {:.2f}\".format(r2))\n",
    "\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "print(\"Train R^2 Score:\", train_r2)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred_test = model.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "print(\"Test R^2 Score:\", test_r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model appears to be overfitting the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
